{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Some of the codes and data in this project are from [Azure website](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-train-models-with-aml) and Andrew Ng's course [Deep Learning](https://www.coursera.org/specializations/deep-learning). Copyright (c) reserved. \n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Convolutional Neural Networks: Hand Sign Classification\n\nIn his project we will implement a ConvNet using TensorFlow to classify the hand sign images into 6 classes: numer 0 to number 5."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 0 - Problem statement: SIGNS Dataset\nThis data is provided by Andrw Ng in his [Deep Learning course](https://www.coursera.org/specializations/deep-learning). It contains about 1000 pictures of hand signs of numbuer 0 to 5. The data format is as below:\n* **Training set**: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).\n* **Test set**: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).\n\nNote that this is a subset of the SIGNS dataset. The complete dataset contains many more signs.\nHere are examples for each number, and how an explanation of how we represent the labels. These are the original pictures, before we lowered the image resolutoion to 64 by 64 pixels. \n<img src=\"https://i.postimg.cc/437QtzfP/SIGNS.png\" style=\"width:800px;\">"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 1 - Set up the development environment\nIn this step we will set up the Azure machine learning development environment."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.1 Import packages\nWe will import Python packages we need in this session. Also display the Azure Machine Learning SDK version:"
    },
    {
      "metadata": {
        "tags": [
          "install"
        ],
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport azureml\nfrom azureml.core import Workspace, Run\n\n# check core SDK version number\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Azure ML SDK Version:  1.0.6\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.2 Connect to a workspace\nCreate a workspace object from the existing workspace. Workspace.from_config() reads the file **config.json** and loads the details into an object named ws:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# load workspace configuration from the config.json file in the current folder.\nws = Workspace.from_config()\nprint(ws.name, ws.location, ws.resource_group, sep = '\\t')",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/Model/config.json\ndocs-ws\twesteurope\tdocs-aml\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.3 Create an experiment\nCreate an experiment to track the runs in your workspace. A workspace can have multiple experiments:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "experiment_name = 'CNN-handsign'\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1.4 Create or attach an existing AMlCompute\nBy using Azure Machine Learning Compute (AmlCompute), a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this project, we create AmlCompute as our training environment. This code creates the compute clusters for us if they don't already exist in our workspace.\n\nCreation of the compute takes about five minutes. If the compute is already in the workspace, this code uses it and skips the creation process:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\ncompute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\ncompute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\ncompute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n\n# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\nvm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n                                                                min_nodes = compute_min_nodes, \n                                                                max_nodes = compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n     # For a more detailed view of current AmlCompute status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "found compute target. just use it. cpucluster\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now have the necessary packages and compute resources to train a model in the cloud."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 2 - Machine learning model and data preparation\n\nWe use Convolutional Neural Network (CNN) momdel as the machine learning model in this project. CNN, or ConvNet is a class of deep neural networks, most commonly applied to analyzing visual imagery. In this session we will do some preapration work such as loading and transforming data.\n### 2.1 - Data preparation\n\nAs usual, we will start by loading in the packages. The package cnn_utils contains the functions such as load data and one hot conversion. The file cnn_utils.py must be included in the sample folder as this notebook."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import math\nimport numpy as np\nimport h5py\nimport scipy\nfrom scipy import ndimage\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom cnn_utils import *\n",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next cell is to load the \"SIGNS\" dataset we are going to use."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Loading the data (signs)\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset('datasets')",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of index below and re-run to see different examples. "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Example of a picture\nindex = 6\nplt.imshow(X_train_orig[index])\nprint (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": "y = 2\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X_train = X_train_orig/255.\nX_test = X_test_orig/255.\nY_train = convert_to_one_hot(Y_train_orig, 6).T\nY_test = convert_to_one_hot(Y_test_orig, 6).T\nprint (\"number of training examples = \" + str(X_train.shape[0]))\nprint (\"number of test examples = \" + str(X_test.shape[0]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(Y_test.shape))\nconv_layers = {}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2.2 Upload data to the cloud\nNow we will make the data accessible remotely by uploading that data from local machine into Azure. Then it can be accessed for remote training. The datastore is a convenient construct associated with our workspace for us to upload or download data. We can also interact with it from your remote compute targets. It's backed by an Azure Blob storage account.\n\nThe SIGNS files are uploaded into a directory named signs at the root of the datastore."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\n\nds.upload(src_dir='./datasets', target_path='signs', overwrite=True, show_progress=True)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "AzureBlob docsws9812269208 azureml-blobstore-4dbb6ed5-5997-46a3-851b-0f3889b3a767\nUploading ./datasets/test_signs.h5\nUploading ./datasets/train_signs.h5\nUploaded ./datasets/test_signs.h5, 1 files out of an estimated total of 2\nUploaded ./datasets/train_signs.h5, 2 files out of an estimated total of 2\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "$AZUREML_DATAREFERENCE_71a64abe35634bfbb1aac639e9b23649"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 3 Train on a remote cluster\n###  3.1 Create a directory\nCreate a directory to deliver the necessary code from local computer to the remote resource:  "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nscript_folder = './CNN-handsign'\nos.makedirs(script_folder, exist_ok=True)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.2 Create a training script\nTo submit the job to the cluster, first create a training script. Run the following code to create the training script called train.py in the directory we created."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%%writefile $script_folder/train.py   \n\nimport math\nimport numpy as np\nimport h5py\nimport scipy\nfrom scipy import ndimage\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom cnn_utils import *\nimport argparse\nimport os\nfrom azureml.core import Run\nimport pickle\n\nnp.random.seed(1)\n\n\n# CNN model functions\ndef create_placeholders(n_H0, n_W0, n_C0, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_H0 -- scalar, height of an input image\n    n_W0 -- scalar, width of an input image\n    n_C0 -- scalar, number of channels of the input\n    n_y -- scalar, number of classes\n        \n    Returns:\n    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n    \"\"\"\n\n    X =  tf.placeholder(\"float\", [None, n_H0, n_W0, n_C0])\n    Y =  tf.placeholder(\"float\", [None, n_y])\n    \n    return X, Y\n\ndef initialize_parameters():\n    \"\"\"\n    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [4, 4, 3, 8]\n                        W2 : [2, 2, 8, 16]\n    Returns:\n    parameters -- a dictionary of tensors containing W1, W2\n    \"\"\"\n    \n    tf.set_random_seed(1)                             \n        \n    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n\n    parameters = {\"W1\": W1,\n                  \"W2\": W2}\n    \n    return parameters\n\n    \n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n    # CONV2D: stride of 1, padding 'SAME'\n    Z1 = tf.nn.conv2d(X, W1, strides = [1,1,1,1], padding = 'SAME')\n    # RELU\n    A1 = tf.nn.relu(Z1)\n    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n    # CONV2D: filters W2, stride 1, padding 'SAME'\n    Z2 = tf.nn.conv2d(P1, W2, strides = [1,1,1,1], padding = 'SAME')\n    # RELU\n    A2 = tf.nn.relu(Z2)\n    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n    # FLATTEN\n    P2 = tf.contrib.layers.flatten(P2)\n    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn = None)\n\n    return Z3\n\n\ndef compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\n    \n    return cost\n\n\n\ndef model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n          num_epochs = 300, minibatch_size = 64, print_cost = True):\n    \"\"\"\n    Implements a three-layer ConvNet in Tensorflow:\n    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n    \n    Arguments:\n    X_train -- training set, of shape (None, 64, 64, 3)\n    Y_train -- test set, of shape (None, n_y = 6)\n    X_test -- training set, of shape (None, 64, 64, 3)\n    Y_test -- test set, of shape (None, n_y = 6)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    train_accuracy -- real number, accuracy on the train set (X_train)\n    test_accuracy -- real number, testing accuracy on the test set (X_test)\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n    seed = 3                                          # to keep results consistent (numpy seed)\n    (m, n_H0, n_W0, n_C0) = X_train.shape             \n    n_y = Y_train.shape[1]                            \n    costs = []                                        # To keep track of the cost\n    \n    # Create Placeholders of the correct shape\n    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n    \n    # Initialize parameters\n    parameters = initialize_parameters()\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    Z3 = forward_propagation(X, parameters)\n    \n    # Cost function: Add cost function to tensorflow graph\n    cost = compute_cost(Z3, Y)\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n    optimizer =  tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n    \n    # Initialize all the variables globally\n    init = tf.global_variables_initializer()\n     \n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n            \n            minibatch_cost = 0.\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                \n                minibatch_cost += temp_cost / num_minibatches\n                \n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 5 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n            if print_cost == True and epoch % 1 == 0:\n                costs.append(minibatch_cost)\n                \n\n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # Calculate the correct predictions\n        predict_op = tf.argmax(Z3, 1)\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n        \n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        print(accuracy)\n        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n        print(\"Train Accuracy:\", train_accuracy)\n        print(\"Test Accuracy:\", test_accuracy)\n        final_param = {}\n        final_param['W1'] = parameters['W1'].eval()      \n        final_param['W2'] = parameters['W2'].eval()   \n        return train_accuracy, test_accuracy, final_param\n\n\n# let user feed in 1 parameter, the location of the data files (from datastore)\nparser = argparse.ArgumentParser()\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\nargs = parser.parse_args()\n\ndata_folder = args.data_folder + '/signs'\nprint('Data folder:', data_folder)\n\n\n# load train and test set into numpy arrays\n# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\nX_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset(data_folder)\nX_train = X_train_orig/255.\nX_test = X_test_orig/255.\nY_train = convert_to_one_hot(Y_train_orig, 6).T\nY_test = convert_to_one_hot(Y_test_orig, 6).T\n\n# get hold of the current run\nrun = Run.get_context()\n    \n# Train the model   \ntrain_accuracy, test_accuracy, final_param = model(X_train, Y_train, X_test, Y_test)\n\n# Result logging\nrun.log('train_accuracy', np.float(train_accuracy))\nrun.log('test_accuracy', np.float(test_accuracy))\n\nos.makedirs('outputs', exist_ok=True)\n# note file saved in the outputs folder is automatically uploaded into experiment record\nf = open('outputs/hand-sign-classification.pkl','wb')\npickle.dump(final_param,f)\nf.close()",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Overwriting ./CNN-handsign/train.py\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Notice how the script gets data and saves models:\n* The training script reads an argument to find the directory that contains the data. When we submit the job later, we point to the datastore for this argument: \n```\nparser.add_argument('--data-folder', type=str, dest='data_folder', help='data directory mounting point')\n```\n* the training script saves our model into a directory named **outputs**: \n```\nf = open('outputs/hand-sign-classification.pkl','wb')\n```\nAnything written in this directory is automatically uploaded into our workspace. We access our model from this directory later in the tutorial. The file cnn_utils.py is referenced from the training script to load the dataset correctly. Now we copy this script into the script folder, so that it can be accessed along with the training script on the remote resource."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import shutil\nshutil.copy('cnn_utils.py', script_folder)",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 69,
          "data": {
            "text/plain": "'./CNN-handsign/cnn_utils.py'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.3 Create an estimator\nAn estimator object is used to submit the run.  Create the estimator by running the following code to define:\n* The name of the estimator object, `est` \n* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution.\n* The compute target.  In this case you will use the AmlCompute you created\n* The training script name, train.py\n* Parameters required from the training script\n* Python packages needed for training\n\nIn this tutorial, this target is AmlCompute. All files in the script folder are uploaded into the cluster nodes for execution. The data_folder is set to use the datastore `ds.as_mount()`.\n   "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.as_mount(),\n}\n\nest = Estimator(source_directory=script_folder,\n                script_params=script_params,\n                compute_target=compute_target,\n                entry_script='train.py',\n                conda_packages=['tensorflow','matplotlib'])",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.4 Submit the job to the cluster\nRun the experiment by submitting the estimator object:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run = exp.submit(config=est)\nrun",
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 71,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>CNN-handsign</td><td>CNN-handsign_1548010870752</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/65c2cf5a-b718-4bd4-9031-b7a2f2881ff0/resourceGroups/docs-aml/providers/Microsoft.MachineLearningServices/workspaces/docs-ws/experiments/CNN-handsign/runs/CNN-handsign_1548010870752\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: CNN-handsign,\nId: CNN-handsign_1548010870752,\nType: azureml.scriptrun,\nStatus: Queued)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Because the call is asynchronous, it returns a Preparing or Running state as soon as the job is started."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3.5 Monitor a remote run\nIn total, the first run takes about 15 minutes. But for subsequent runs, as long as the script dependencies don't change, the same image is reused. So the container startup time is much faster.\nWhat happens while we wait:\n\n* **Image creation**: A Docker image is created that matches the Python environment specified by the estimator. The image is uploaded to the workspace. \n\n    This stage happens once for each Python environment because the container is cached for subsequent runs. During image creation, logs are streamed to the \n    run history. The image creation progress can be monitored by using these logs.\n    \n\n* **Scaling**: If the remote cluster requires more nodes to do the run than currently available, additional nodes are added automatically. Scaling typically takes about five minutes.\n\n\n* **Running**: In this stage, the necessary scripts and files are sent to the compute target. Then datastores are mounted or copied. And then the entry_script is run. While the job is running, stdout and the ./logs directory are streamed to the run history. We can monitor the run's progress by using these logs. \n\n\n* **Post-processing**: The ./outputs directory of the run is copied over to the run history in the workspace, so we can access these results.\n\nWe can check the progress of a running job in several ways. In this project we use a Jupyter widget and a `wait_for_completion` method.\n\nThe code below is the running progress checking using Jupyter widget. Like the run submission, the widget is asynchronous and provides live updates every 10 to 15 seconds until the job finishes."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(run).show()",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80cebff323f40748764ec421fef2f57",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The code below is the running progress checking using `wait_for_completion` method. Model training and monitoring happen in the background. Wait until the model has finished training before running more code."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.wait_for_completion(show_output=False) # specify True for a verbose log",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 67,
          "data": {
            "text/plain": "{'runId': 'CNN-handsign_1548009717015',\n 'target': 'cpucluster',\n 'status': 'Failed',\n 'startTimeUtc': '2019-01-20T18:48:45.598105Z',\n 'endTimeUtc': '2019-01-20T18:55:49.442324Z',\n 'properties': {'azureml.runsource': 'experiment',\n  'ContentSnapshotId': '422af809-213d-4923-8be3-2c6dfb961a1a'},\n 'runDefinition': {'Script': 'train.py',\n  'Arguments': ['--data-folder', '$AZUREML_DATAREFERENCE_workspaceblobstore'],\n  'SourceDirectoryDataStore': None,\n  'Framework': 0,\n  'Communicator': 0,\n  'Target': 'cpucluster',\n  'DataReferences': {'workspaceblobstore': {'DataStoreName': 'workspaceblobstore',\n    'Mode': 'Mount',\n    'PathOnDataStore': None,\n    'PathOnCompute': None,\n    'Overwrite': False}},\n  'JobName': None,\n  'AutoPrepareEnvironment': True,\n  'MaxRunDurationSeconds': None,\n  'NodeCount': 1,\n  'Environment': {'Python': {'InterpreterPath': 'python',\n    'UserManagedDependencies': False,\n    'CondaDependencies': {'name': 'project_environment',\n     'dependencies': ['python=3.6.2',\n      {'pip': ['azureml-defaults']},\n      'tensorflow',\n      'matplotlib']}},\n   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base:0.2.0',\n    'Enabled': True,\n    'SharedVolumes': True,\n    'Preparation': None,\n    'GpuSupport': False,\n    'ShmSize': '1g',\n    'Arguments': [],\n    'BaseImageRegistry': {'Address': None,\n     'Username': None,\n     'Password': None}},\n   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n    'Packages': [{'Group': 'com.microsoft.ml.spark',\n      'Artifact': 'mmlspark_2.11',\n      'Version': '0.12'}],\n    'PrecachePackages': True}},\n  'History': {'OutputCollection': True},\n  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n    'spark.yarn.maxAppAttempts': '1'}},\n  'BatchAi': {'NodeCount': 0},\n  'AmlCompute': {'Name': None,\n   'VmSize': None,\n   'VmPriority': None,\n   'RetainCluster': False,\n   'ClusterMaxNodeCount': 1},\n  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n  'Mpi': {'ProcessCountPerNode': 1},\n  'Hdi': {'YarnDeployMode': 2},\n  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n  'ExposedPorts': None,\n  'PrepareEnvironment': None},\n 'logFiles': {'azureml-logs/60_control_log.txt': 'https://docsws9812269208.blob.core.windows.net/azureml/ExperimentRun/dcid.CNN-handsign_1548009717015/azureml-logs/60_control_log.txt?sv=2018-03-28&sr=b&sig=QZ1M6eGS%2F3kOmrX74MBXEG2JhQgOXatoHhamV5Sp9YI%3D&st=2019-01-20T18%3A45%3A57Z&se=2019-01-21T02%3A55%3A57Z&sp=r',\n  'azureml-logs/80_driver_log.txt': 'https://docsws9812269208.blob.core.windows.net/azureml/ExperimentRun/dcid.CNN-handsign_1548009717015/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=8V7moMupbUuOKLuHJA7CuB1QjwkRT968In%2FBHn6QRao%3D&st=2019-01-20T18%3A45%3A57Z&se=2019-01-21T02%3A55%3A57Z&sp=r',\n  'azureml-logs/azureml.log': 'https://docsws9812269208.blob.core.windows.net/azureml/ExperimentRun/dcid.CNN-handsign_1548009717015/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=SDGliqsZg1At3fEa2wFzFjW02VaEJeJWs9rU2DrqDpg%3D&st=2019-01-20T18%3A45%3A57Z&se=2019-01-21T02%3A55%3A57Z&sp=r',\n  'azureml-logs/56_batchai_stderr.txt': 'https://docsws9812269208.blob.core.windows.net/azureml/ExperimentRun/dcid.CNN-handsign_1548009717015/azureml-logs/56_batchai_stderr.txt?sv=2018-03-28&sr=b&sig=iwZWeqFEIfiRXAHvtOh5FvA4UcbQP5dJkRdfERY0tK4%3D&st=2019-01-20T18%3A45%3A57Z&se=2019-01-21T02%3A55%3A57Z&sp=r',\n  'azureml-logs/55_batchai_execution.txt': 'https://docsws9812269208.blob.core.windows.net/azureml/ExperimentRun/dcid.CNN-handsign_1548009717015/azureml-logs/55_batchai_execution.txt?sv=2018-03-28&sr=b&sig=x8%2FbPI4AULcPaL%2BkmXLM6yetAkIMeqxuUlTMsaTRn%2Bc%3D&st=2019-01-20T18%3A45%3A57Z&se=2019-01-21T02%3A55%3A57Z&sp=r'}}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We now have a model trained on a remote cluster. Retrieve the accuracy of the model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_metrics())",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": "{'train_accuracy': 0.7768518328666687, 'test_accuracy': 0.6583333611488342}\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 4 Register model\nThe last step in the training script wrote the file `outputs/hand-sign-classification.pkl` in a directory named `outputs` in the VM of the cluster where the job is run. `outputs` is a special directory in that all content in this directory is automatically uploaded to the workspace. This content appears in the run record in the experiment under the workspace. So the model file is now also available in the workspace.\n\nWe can see files associated with that run:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(run.get_file_names())",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'outputs/hand-sign-classification.pkl', 'azureml-logs/azureml.log', 'azureml-logs/55_batchai_execution.txt']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now we register the model in the workspace, so that we or other collaborators can later query, examine, and deploy this model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# register model \nmodel = run.register_model(model_name='hand-sign-classification', model_path='outputs/hand-sign-classification.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": "hand-sign-classification\thand-sign-classification:1\t1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 5 Clean up resources\nIf we don't plan to use the resources we created, we can delete them, so we don't incur any charges:\n1. In the Azure portal, select **Resource groups** on the far left.\n<img src=\"https://i.postimg.cc/44QmgNzN/post-azure5.png\" style=\"width:800px;\">\n2. From the list, select the resource group you created.\n3. Select **Delete resource group**.\n4. Enter the resource group name. Then select **Delete**."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We can also delete just the Azure Machine Learning Compute cluster. However, autoscale is turned on, and the cluster minimum is zero. So this particular resource won't incur additional compute charges when not in use:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# optionally, delete the Azure Machine Learning Compute cluster\ncompute_target.delete()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "roastala"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}